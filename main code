import requests
from bs4 import BeautifulSoup
import numpy as np
#發票週期
year_date = str(input("請輸入查詢年和奇數月:"))
# year_date = '11107'
if len(year_date)<=5:

    url = 'https://www.etax.nat.gov.tw/etw-main/ETW183W2_' + year_date
    headers = {'useragent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64} AppleWebKit/537.36(KHTML, likeGecko) Chrome/65.0.3342.181 Safari/537.36'}  #授權
    resp = requests.get(url, headers = headers)
    resp.encoding = 'utf-8'
    html_doc = resp.text
    soup = BeautifulSoup(html_doc, "lxml")   #解析器'lxml' //html.parser
    num_tags = soup.find_all("td")  #搜td的屬性
    p_tags = num_tags[5].find_all('div',class_="col-12 mb-3") #在td 和共通col-12 mb-3
    print('發票週期', year_date)
    print('特別獎:', num_tags[1].text.lstrip())
    print('特獎:', num_tags[3].text.lstrip())
    print('頭獎:', p_tags[0].text,p_tags[1].text.lstrip(), p_tags[2].text.lstrip())

    A = p_tags[0].text
    B = p_tags[1].text
    C = p_tags[2].text
    # print(type(A))
    # print(A)
    a = A.lstrip()
    b = B.lstrip()
    c = C.lstrip()
    # print(a[3])


    new = int(year_date)


    if new <= 11011:
        print('增開六獎:',a[5:8])
        print('增開六獎:',b[5:8])
        print('增開六獎:',c[5:8])
#...........................................................dune
import requests
import json
import pandas as pd
import numpy as np
from bs4 import BeautifulSoup
from selenium import webdriver
# driver = webdriver.Chrome("D:\PycharmProjects\web crawler\chromedriver.exe")
url = 'https://app-api.dune.com/v1/graphql'
headers = {'user-agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Mobile Safari/537.36'}
myobj={"operationName":"GetExecution","variables":{"execution_id":"01GNYKRWFD68KJE85XSJCSJEQD","query_id":317440,"parameters":[]},"query":"query GetExecution($execution_id: String!, $query_id: Int!, $parameters: [Parameter!]!) {\n  get_execution(\n    execution_id: $execution_id\n    query_id: $query_id\n    parameters: $parameters\n  ) {\n    execution_queued {\n      execution_id\n      execution_user_id\n      position\n      execution_type\n      created_at\n      __typename\n    }\n    execution_running {\n      execution_id\n      execution_user_id\n      execution_type\n      started_at\n      created_at\n      __typename\n    }\n    execution_succeeded {\n      execution_id\n      runtime_seconds\n      generated_at\n      columns\n      data\n      __typename\n    }\n    execution_failed {\n      execution_id\n      type\n      message\n      metadata {\n        line\n        column\n        hint\n        __typename\n      }\n      runtime_seconds\n      generated_at\n      __typename\n    }\n    __typename\n  }\n}\n"}

response = requests.post(url, json=myobj, headers=headers)

content = response.text
y = json.loads(content)
# print(y)
# print(y['data'])
# print(y['data']['get_execution'])
# print(y['data']['get_execution']['execution_succeeded'])
# print(y['data']['get_execution']['execution_succeeded']['data'])
datalist= y['data']['get_execution']['execution_succeeded']['data']
print(type(datalist))
print((datalist))
daylist=[]
tradelist=[]
for i in datalist:
    daylist.append(i['day'])
    tradelist.append(i['trader_'])
print(daylist)
print(len(daylist))
print(tradelist)
print(type(tradelist))
print(tradelist[1])
dataoutput = open ("data.xls",'w')
dataoutput.write('D:\PycharmProjects\web crawler')
for i  in range(len(tradelist)):
    # for j in range(len(tradelist[i])):
    dataoutput.write(str(tradelist[i]))
    dataoutput.write('\t')
    dataoutput.write('\n')
dataoutput.close()
